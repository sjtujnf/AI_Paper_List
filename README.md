# Table of Contents


 - [About This Project](#about-this-project)
 - [Some Statistics of the Papers](#some-statistics-of-the-papers)
 - [The Chronological Listing of Papers](#the-chronological-listing-of-papers)


# About This Project

This project aims to collect and summarize the AI-related papers for readers who are interested in AI research in academia. We plan to collect all the AI-related papers in the top-tier architecture conferences such as ISCA, MICRO and HPCA in recent years. Now, we have collected them in ISCA from 2015 to 2019 with some basic analysis. These papers will be listed below and you can find our brief summaries in "/Summarys/#year_of_the_paper/". We are glad to have your suggestions of anything about this project!

# Some Statistics of the Papers
## 1.	The yearly paper count (now only based on ISCA 2015-2019 statistics)
  
<div align="center"><img width="600" src="uploads/QQ截图20200313174817.jpg"/></div><br>
The trend of AI is generaly increasing. But now it slightly slow down in 2019. And we can find out that year 2018 takes almost half of the counts, implicating the hottest year of AI accelerators.
<br>

## 2.	The countries and regions that contribute (now only based on ISCA 2015-2019 statistics)

<div align="center"><img width="600" src="uploads/QQ截图20200313174833.jpg"/></div><br>
America is definitely the origin area of most papers. China and North Korea are still two chasing character in AI research though they have done somg terrific ahievements.
<br>

## 3. Top researchers and their information (now only based on ISCA 2015-2019 statistics)

<br>
Here are the names appear most frequently on the collected papers. We collect thier public information and list below to help you find the leader researchers in this area. <br>

|  Rank  |  Author  |	 Counts of paper  | 	Region  | 	Lab or Corp.  |
|  ----  | ----  |  ----  |  ----  | ----  |
| 1 | Hadi Esmaeilzadeh |	4	| US	| Alternative Computing Technologies (ACT) Laboratory, University of California |
| 2 | Mingcong Song	| 3 |	US	| Intelligent Design of Efficient Architectures Laboratory (IDEAL), University of Florida |
| 2 | Reetuparna Das |	3	| US	| EECS department, University of Michigan |
| 2 | Tao Li |	3 |	US |	Intelligent Design of Efficient Architectures Laboratory (IDEAL), University of Florida |
| 2 | Tianshi Chen |	3 | China	| Cambricon Technologies Corporation Limited(寒武纪科技) |
| 2 | Yunji Chen |	3 | China	| Institute of Computing Technology, Chinese Academy of Sciences |
| 2 | Zidong Du |	3 | China	| Institute of Computing Technology, Chinese Academy of Sciences |

<br>


<!--
# Tech Poins Summary
The figure below summarys how every architechture proposed was implemented to take the experiments.
<br>
<div align="center"><img width="600" src="uploads/QQ截图20200316142634.jpg"/></div><br>
And this figure below summarys the scene accelerators supposed to be used at.
<br>
<div align="center"><img width="600" src="uploads/QQ截图20200316142812.jpg"/></div><br>
<br>
-->

# The Chronological Listing of Papers


<br>
Now we list all the papers we have collected. If it is linkable, it is linked to the summary of the paper and the summaries are still updating.<br>

# ISCA 
## 2015
<table>
  <tr>
    <th text-align="center">
    <th text-align="center">Title
    <th text-align="center">Authors
    <th text-align="center">Area
    <th text-align="center">Organization
  </tr>
  <tr>
    <th text-align="center">1
    <!--
    <th text-align="left">ShiDianNao: Shifting Vision Processing Closer to the Sensor
    -->
    <td><a href="Summarys/2015/ShiDianNao Shifting Vision Processing Closer to the Sensor.docx">ShiDianNao: Shifting Vision Processing Closer to the Sensor</a></td>
    <th text-align="left">Zidong Du
    <th text-align="center">China
    <th text-align="left">ICT
  </tr>
</table>
<br>

## 2016
<table>
  <tr>
    <th text-align="center">
    <th text-align="center">Title
    <th text-align="center">Authors
    <th text-align="center">Area
    <th text-align="center">Organization
  </tr>
  <tr>
    <th text-align="center">1
    <!--
    <th text-align="left">Cnvlutin: Ineffectual-Neuron-Free Deep Neural Network Computing 
    -->
    <td><a href="Summarys/2016/Cnvlutin Ineffectual-Neuron-Free Deep Neural Network Computing .docx">Cnvlutin: Ineffectual-Neuron-Free Deep Neural Network Computing</a></td>
    <th text-align="left">Jorge Albericio, Tayler Hetheringto
    <th text-align="center">Canada
    <th text-align="left">University of Toronto, University of British Columbia
  </tr>
  <tr>
    <th text-align="center">2
    <th text-align="left">ISAAC: A Convolutional Neural Network Accelerator with In-Situ Analog Arithmetic in Crossbars 
    <th text-align="left">Ali Shafiee, Vivek Srikumar
    <th text-align="center">US
    <th text-align="left">University of Utah，Hewlett Packard Labs
  </tr>
  <tr>
    <th text-align="center">3
    <th text-align="left">PRIME: A Novel Processing-in-Memory Architecture for Neural Network Computation in ReRAM-Based Main Memory
    <th text-align="left">Ping Chi, Yuan Xie
    <th text-align="center">US
    <th text-align="left">University of California 
  </tr>
  <tr>
    <th text-align="center">4
    <th text-align="left">EIE: Efficient Inference Engine on Compressed Deep Neural Network
    <th text-align="left">Song Han, William J. Dally
    <th text-align="center">US
    <th text-align="left">Stanford University, NVIDIA
  </tr>
  <tr>
    <th text-align="center">5
    <th text-align="left">RedEye: Analog ConvNet Image Sensor Architecture for Continuous Mobile 
    <th text-align="left">Robert LiKamWa, Lin Zhong
    <th text-align="center">US
    <th text-align="left">Rice University
  </tr>
  <tr>
    <th text-align="center">6
    <!--
    <th text-align="left">Minerva: Enabling Low-Power, Highly-Accurate Deep Neural Network Accelerators 
    -->
    <td><a href="Summarys/2016/Minerva Enabling Low-Power, Highly-Accurate Deep Neural Network Accelerators .docx">Minerva: Enabling Low-Power, Highly-Accurate Deep Neural Network Accelerators</a></td>
    <th text-align="left">Brandon Reagen, David Brooks
    <th text-align="center">US
    <th text-align="left">Harvard University 
  </tr>
  <tr>
    <th text-align="center">7
    <th text-align="left">Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks 
    <th text-align="left">Yu-Hsin Chen, Vivienne Sze
    <th text-align="center">US
    <th text-align="left">MIT, NVIDIA
  </tr>
  <tr>
    <th text-align="center">8
    <!--
    <th text-align="left">Neurocube: A Programmable Digital Neuromorphic Architecture with High-Density 3D Memory 
    -->
    <td><a href="Summarys/2016/Neurocube A Programmable Digital Neuromorphic Architecture with High-Density 3D Memory.docx">Neurocube: A Programmable Digital Neuromorphic Architecture with High-Density 3D Memory</a></td>
    <th text-align="left">Duckhwan Kim, Saibal Mukhopadhyay
    <th text-align="center">US
    <th text-align="left">Georgia Institute of Technology 
  </tr>
  <tr>
    <th text-align="center">9
    <!--
    <th text-align="left">Cambricon: An Instruction Set Architecture for Neural Networks 
    -->
    <td><a href="Summarys/2016/Cambricon An Instruction Set Architecture for Neural Networks.docx">Cambricon: An Instruction Set Architecture for Neural Networks</a></td>
    <th text-align="left">Shaoli Liu, Tianshi Chen
    <th text-align="center">China
    <th text-align="left">CAS, Cambricon Ltd.
  </tr>
  <tr>
    <th text-align="center">10
    <!--
    <th text-align="left">Energy Efficient Architecture for Graph Analytics Accelerators 
    -->
    <td><a href="Summarys/2016/Energy Efficient Architecture for Graph Analytics Accelerators.docx">Energy Efficient Architecture for Graph Analytics Accelerators</a></td>
    <th text-align="left">Muhammet Mustafa Ozdal, Ozcan Ozturk
    <th text-align="center">Turkey
    <th text-align="left">Bilkent University
  </tr>
  <tr>
    <th text-align="center">11
    <!--
    <th text-align="left">Accelerating Markov Random Field Inference Using Molecular Optical Gibbs Sampling Units 
    -->
    <td><a href="Summarys/2016/Accelerating Markov Random Field Inference Using Molecular Optical Gibbs Sampling Units.docx">Accelerating Markov Random Field Inference Using Molecular Optical Gibbs Sampling Units</a></td>
    <th text-align="left">Siyang Wang, Alvin R. Lieberk
    <th text-align="center">US
    <th text-align="left">Duke University
  </tr>
</table>
<br>

## 2017
<table>
  <tr>
    <th text-align="center">
    <th text-align="center">Title
    <th text-align="center">Authors
    <th text-align="center">Area
    <th text-align="center">Organization
  </tr>
  <tr>
    <th text-align="center">1
    <th text-align="left">In-Datacenter Performance Analysis of a Tensor Processing Unit
    <th text-align="left">Norman P. Jouppi
    <th text-align="center">US
    <th text-align="left">Google
  </tr>
  <tr>
    <th text-align="center">2
    <th text-align="left">Maximizing CNN Accelerator Efficiency Through Resource Partitioning
    <th text-align="left">Yongming Shen
    <th text-align="center">US
    <th text-align="left">Stony Brook University
  </tr>
  <tr>
    <th text-align="center">3
    <th text-align="left">SCALEDEEP: A Scalable Compute Architecture for Learning and Evaluating Deep Networks
    <th text-align="left">Swagath Venkataramani, Anand Raghunathan
    <th text-align="center">US
    <th text-align="left">Purdue University, Parallel Computing Lab, Intel Corporation
  </tr>
  <tr>
    <th text-align="center">4
    <th text-align="left">Scalpel: Customizing DNN Pruning to the Underlying Hardware Parallelism
    <th text-align="left">Jiecao Yu, Scott Mahlke
    <th text-align="center">US
    <th text-align="left">University of Michigan, ARM
  </tr>
  <tr>
    <th text-align="center">5
    <!--
    <th text-align="left">SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks
    -->
    <td><a href="Summarys/2017/SCNN An Accelerator for Compressed-sparse Convolutional Neural Networks.docx">SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks</a></td>
    <th text-align="left">Angshuman Parashar, William J. Dally
    <th text-align="center">US
    <th text-align="left">NVIDIA, MIT, UC-Berkeley, Stanford University
  </tr>
  <tr>
    <th text-align="center">6
    <th text-align="left">Stream-Dataflow Acceleration
    <th text-align="left">Tony Nowatzki
    <th text-align="center">US
    <th text-align="left">University of California, University of Wisconsin
  </tr>
  <tr>
    <th text-align="center">7
    <th text-align="left">Understanding and Optimizing Asynchronous Low-Precision Stochastic Gradient Descent
    <th text-align="left">Christopher De Sa, Kunle Olukotun
    <th text-align="center">US
    <th text-align="left">Stanford University
  </tr>
</table>
<br>

## 2018
<table>
  <tr>
    <th text-align="center">
    <th text-align="center">Title
    <th text-align="center">Authors
    <th text-align="center">Area
    <th text-align="center">Organization
  </tr>
  <tr>
    <th text-align="center">1
    <!--
    <th text-align="left">A Configurable Cloud-Scale DNN Processor for Real-Time AI 
    -->
    <td><a href="Summarys/2018/A Configurable Cloud-Scale DNN Processor for Real-Time AI.docx">A Configurable Cloud-Scale DNN Processor for Real-Time AI</a></td>
    <th text-align="left">Jeremy Fowers, Doug Burger
    <th text-align="center">US
    <th text-align="left">Microsoft 
  </tr>
  <tr>
    <th text-align="center">2
    <th text-align="left">PROMISE: An End-to-End Design of a Programmable Mixed-Signal Accelerator for Machine- Learning Algorithms 
    <th text-align="left">Prakalp Srivastava, Mingu Kang
    <th text-align="center">US
    <th text-align="left">University of Illinois at Urbana-Champaign, IBM
  </tr>
  <tr>
    <th text-align="center">3
    <th text-align="left">Computation Reuse in DNNs by Exploiting Input Similarity 
    <th text-align="left">Marc Riera, Antonio Gonza ?lez
    <th text-align="center">Spain
    <th text-align="left">Universitat Polite ?cnica de Catalunya 
  </tr>
  <tr>
    <th text-align="center">4
    <!-- 
    <th text-align="left">GenAx: A Genome Sequencing Accelerator 
    -->
    <td><a href="Summarys/2018/GANAX A Unified MIMD-SIMD Acceleration for Generative Adversarial Networks.docx">GenAx: A Genome Sequencing Accelerator</a></td>
    <th text-align="left">Daichi Fujiki, Satish Narayanasamy
    <th text-align="center">US
    <th text-align="left">University of Michigan 
  </tr>
  <tr>
    <th text-align="center">5
    <th text-align="left">Flexon: A Flexible Digital Neuron for Efficient Spiking Neural Network Simulations 
    <th text-align="left">Dayeol Lee, Jangwoo Kim
    <th text-align="center">North Korea,US
    <th text-align="left">Seoul National University, University of California
  </tr>
  <tr>
    <th text-align="center">6
    <th text-align="left">Space-Time Algebra: A Model for Neocortical Computation 
    <th text-align="left">James E. Smith 
    <th text-align="center">US
    <th text-align="left">University of Wisconsin-Madison 
  </tr>
  <tr>
    <th text-align="center">7
    <th text-align="left">Architecting a Stochastic Computing Unit with Molecular Optical Devices 
    <th text-align="left">Xiangyu Zhang, Alvin R. Lebeck 
    <th text-align="center">US
    <th text-align="left">Duke University, Parabon Labs
  </tr>
  <tr>
    <th text-align="center">8
    <!--
    <th text-align="left">RANA: Towards Efficient Neural Acceleration with Refresh-Optimized Embedded DRAM 
    -->
    <td><a href="Summarys/2018/RANA Towards Efficient Neural Acceleration with Refresh-Optimized Embedded DRAM .docx">RANA: Towards Efficient Neural Acceleration with Refresh-Optimized Embedded DRAM</a></td>
    <th text-align="left">Fengbin Tu, Shaojun Wei
    <th text-align="center">China
    <th text-align="left">Tsinghua University
  </tr>
  <tr>
    <th text-align="center">9
    <th text-align="left">Neural Cache: Bit-Serial In-Cache Acceleration of Deep Neural Networks 
    <th text-align="left">Charles Eckert, Reetuparna Das
    <th text-align="center">US
    <th text-align="left">University of Michigan, Intel Corporation
  </tr>
  <tr>
    <th text-align="center">10
    <th text-align="left">RoboX: An End-to-End Solution to Accelerate Autonomous Control in Robotics 
    <th text-align="left">Jacob Sacks, Hadi Esmaeilzadeh
    <th text-align="center">US
    <th text-align="left">Georgia Institute of Technology, University of California, San Diego
  </tr>
  <tr>
    <th text-align="center">11
    <th text-align="left">EVA2: Exploiting Temporal Redundancy in Live Computer Vision 
    <th text-align="left">Mark Buckler, Adrian Sampson
    <th text-align="center">US
    <th text-align="left">Cornell University 
  </tr>
  <tr>
    <th text-align="center">12
    <!--
    <th text-align="left">Euphrates: Algorithm-SoC Co-Design for Low-Power Mobile Continuous Vision 
    -->
    <td><a href="Summarys/2018/Euphrates Algorithm-SoC Co-Design for Low-Power Mobile Continuous Vision.docx">Euphrates: Algorithm-SoC Co-Design for Low-Power Mobile Continuous Vision</a></td>
    <th text-align="left">Yuhao Zhu, Paul Whatmough
    <th text-align="center">US
    <th text-align="left">University of Rochetster, ARM Research
  </tr>
  <tr>
    <th text-align="center">13
    <!--
    <th text-align="left">GANAX: A Unified MIMD-SIMD Acceleration for Generative Adversarial Networks
    -->
    <td><a href="Summarys/2018/GANAX A Unified MIMD-SIMD Acceleration for Generative Adversarial Networks.docx">GANAX: A Unified MIMD-SIMD Acceleration for Generative Adversarial Networks</a></td>
    <th text-align="left">Amir Yazdanbakhsh, Hadi Esmaeilzadeh
    <th text-align="center">US
    <th text-align="left">Georgia Institute of Technology, UC San Diego, Qualcomm Technologies, Inc.
  </tr>
  <tr>
    <th text-align="center">14
    <!-- 
    <th text-align="left">SnaPEA: Predictive Early Activation for Reducing Computation in Deep Convolutional Neural Networks
    -->
    <td><a href="Summarys/2018/SnaPEA Predictive Early Activation for Reducing Computation in Deep Convolutional Neural Networks.docx">SnaPEA: Predictive Early Activation for Reducing Computation in Deep Convolutional Neural Networks</a></td>
    <th text-align="left">Vahideh Akhlaghi, Hadi Esmaeilzadeh
    <th text-align="center">US
    <th text-align="left">Georgia Institute of Technology, UC San Diego, Qualcomm Technologies, Inc.
  </tr>
  <tr>
    <th text-align="center">15
    <!-- 
    <th text-align="left">UCNN: Exploiting Computational Reuse in Deep Neural Networks via Weight Repetition 
    -->
    <td><a href="Summarys/2018/UCNN Exploiting Computational Reuse in Deep Neural Networks via Weight Repetition.docx">UCNN: Exploiting Computational Reuse in Deep Neural Networks via Weight Repetition</a></td>
    <th text-align="left">Kartik Hegde, Christopher W. Fletche
    <th text-align="center">US
    <th text-align="left">University of Illinois at Urbana-Champaign,  NVIDIA
  </tr>
  <tr>
    <th text-align="center">16
    <!--
    <th text-align="left">Energy-Efficient Neural Network Accelerator Based on Outlier-Aware Low-Precision Computation
    -->
    <td><a href="Summarys/2018/Energy-efficient Neural Network Accelerator Based on Outlier-aware Low-precision Computation.docx">Energy-Efficient Neural Network Accelerator Based on Outlier-Aware Low-Precision Computation</a></td>
    <th text-align="left">Eunhyeok Park, Sungjoo Yoo 
    <th text-align="center">North Korea
    <th text-align="left">Seoul National University 
  </tr>
  <tr>
    <th text-align="center">17
    <th text-align="left">Prediction Based Execution on Deep Neural Networks
    <th text-align="left">Mingcong Song, Tao Li
    <th text-align="center">US
    <th text-align="left">University of Flirida
  </tr>
  <tr>
    <th text-align="center">18
    <th text-align="left">Bit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Network
    <th text-align="left">Hardik Sharma, Hadi Esmaeilzadeh
    <th text-align="center">US
    <th text-align="left">Georgia Institute of Technology, University of California
  </tr>
  <tr>
    <th text-align="center">19
    <th text-align="left">Gist: Efficient Data Encoding for Deep Neural Network Training 
    <th text-align="left">Animesh Jain, Gennady Pekhimenko
    <th text-align="center">US,Canada
    <th text-align="left">Microsoft Research, University of Toronto, Univerity of Michigan
  </tr>
  <tr>
    <th text-align="center">20
    <th text-align="left">The Dark Side of DNN Pruning 
    <th text-align="left">Reza Yazdani, Antonio Gonza ?lez
    <th text-align="center">Spain
    <th text-align="left">Universitat Polite ?cnica de Catalunya 
  </tr>
</table>
<br>

## 2019
<table>
  <tr>
    <th text-align="center">
    <th text-align="center">Title
    <th text-align="center">Authors
    <th text-align="center">Area
    <th text-align="center">Organization
  </tr>
  <tr>
    <th text-align="center">1
    <th text-align="left">3D-based Video Recognition Acceleration by Leveraging Temporal Locality
    <th text-align="left">Huixiang Chen, Tao Li
    <th text-align="center">US
    <th text-align="left">University of Florida
  </tr>
  <tr>
    <th text-align="center">2
    <th text-align="left">A Stochastic-Computing based Deep Learning Framework using Adiabatic Quantum-Flux-Parametron Superconducting Technology
    <th text-align="left">Ruizhe Cai, Ao Ren, Nobuyuki Yoshikawa, Yanzhi Wang
    <th text-align="center">US
    <th text-align="left">Northeastern University
  </tr>
  <tr>
    <th text-align="center">3
    <!--
    <th text-align="left">Accelerating Distributed Reinforcement Learning with In-Switch Computing
    -->
    <td><a href="Summarys/2019/Accelerating Distributed Reinforcement Learning with In-Switch Computing.docx">Accelerating Distributed Reinforcement Learning with In-Switch Computing</a></td>
    <th text-align="left">Youjie Li, Jian Huang
    <th text-align="center">US
    <th text-align="left">UIUC
  </tr>
  <tr>
    <th text-align="center">4
    <!--
    <th text-align="left">Eager Pruning: Algorithm and Architecture Support for Fast Training of Deep Neural Networks
    -->
    <td><a href="Summarys/2019/Eager Pruning Algorithm and Architecture Support for Fast Training of Deep Neural Networks.docx">Eager Pruning: Algorithm and Architecture Support for Fast Training of Deep Neural Networks</a></td>
    <th text-align="left">Jiaqi Zhang, Tao Li
    <th text-align="center">US
    <th text-align="left">University of Florida
  </tr>
  <tr>
    <th text-align="center">5
    <!--
    <th text-align="left">Laconic Deep Learning Inference Acceleration
    -->
    <td><a href="Summarys/2019/Laconic Deep Learning Inference Acceleration.docx">Laconic Deep Learning Inference Acceleration</a></td>
    <th text-align="left">Sayeh Sharify, Andreas Moshovos
    <th text-align="center">Canada
    <th text-align="left">University of Toronto
  </tr>
  <tr>
    <th text-align="center">6
    <!--
    <th text-align="left">MnnFast: A Fast and Scalable System Architecture for Memory-Augmented Neural Networks
    -->
    <td><a href="Summarys/2019/MnnFast A Fast and Scalable System Architecture for Memory-Augmented Neural Network.docx">MnnFast: A Fast and Scalable System Architecture for Memory-Augmented Neural Networks</a></td>
    <th text-align="left">Hanhwi Jang, Jangwoo Kim
    <th text-align="center">North Korea
    <th text-align="left">POSTECH, Seoul National University
  </tr>
  <tr>
    <th text-align="center">7
    <!--
    <th text-align="left">Sparse ReRAM Engine: Joint Exploration of Activation and Weight Sparsity in Compressed Neural Networks
    -->
    <td><a href="Summarys/2019/Sparse ReRAM Engine Joint Exploration of Activation and Weight Sparsity in Compressed Neural Networks.docx">Sparse ReRAM Engine: Joint Exploration of Activation and Weight Sparsity in Compressed Neural Networks</a></td>
    <th text-align="left">Tzu-Hsien Yang
    <th text-align="center">China Twain
    <th text-align="left">National Taiwan University, Academia Sinica, Macronix International Co., Ltd.
  </tr>
  <tr>
    <th text-align="center">8
    <!--
    <th text-align="left">TIE: Energy-efficient Tensor Train-based Inference Engine for Deep Neural Network
    -->
    <td><a href="Summarys/2019/TIE Energy-efficient Tensor Train-based Inference Engine for Deep Neural Network.docx">TIE: Energy-efficient Tensor Train-based Inference Engine for Deep Neural Network</a></td>
    <th text-align="left">Chunhua Deng, Bo Yuan
    <th text-align="center">US
    <th text-align="left">Rutgers University
  </tr>
  <tr>
    <th text-align="center">9
    <!--
    <th text-align="left">FloatPIM_ in-memory acceleration of deep neural network training with high precision
    -->
    <td><a href="Summarys/2019/FloatPIM in-memory acceleration of deep neural network training with high precision.docx">FloatPIM_ in-memory acceleration of deep neural network training with high precision</a></td>
    <th text-align="left">Mohsen Imani, Tajana Rosing
    <th text-align="center">US
    <th text-align="left">UC San Diego
  </tr>
  <tr>
    <th text-align="center">10
    <!--
    <th text-align="left">Cambricon-F_ machine learning computers with fractal von neumann architecture
    -->
    <td><a href="Summarys/2019/Cambricon-F Machine Learning Computers with Fractal von Neumann Architecture.docx">Cambricon-F_ machine learning computers with fractal von neumann architecture</a></td>
    <th text-align="left">Yongwei Zhao, Yunji Chen
    <th text-align="center">China
    <th text-align="left">ICT, Cambricon
  </tr>
  <tr>
    <th text-align="center">11
    <!--
    <th text-align="left">Master of none acceleration_ a comparison of accelerator architectures for analytical query processing
    -->
    <td><a href="Summarys/2019/Master of None Acceleration A Comparison of Accelerator Architectures for Analytical Query Processing.docx">Master of none acceleration_ a comparison of accelerator architectures for analytical query processing</a></td>
    <th text-align="left">Andrea Lottarini, Martha A. Kim
    <th text-align="center">US
    <th text-align="left">Google, Columbia University
  </tr>
</table>


# ASPLOS
## 2014
- DianNao: A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning. (CAS, Inria)

## 2015
- PuDianNao: A Polyvalent Machine Learning Accelerator. (CAS, USTC, Inria)

## 2017
- Tetris: Scalable and Efficient Neural Network Acceleration with 3D Memory. (Stanford University)
- SC-DCNN: Highly-Scalable Deep Convolutional Neural Network using Stochastic Computing. (Syracuse University, USC, The City College of New York)

## 2018
- Bridging the Gap Between Neural Networks and Neuromorphic Hardware with A Neural Network Compiler. (Tsinghua, UCSB)
- MAERI: Enabling Flexible Dataflow Mapping over DNN Accelerators via Reconfigurable Interconnects. (Georgia Tech)
- VIBNN: Hardware Acceleration of Bayesian Neural Networks. (Syracuse University, USC)
- Exploiting Dynamical Thermal Energy Harvesting for Reusing in Smartphone with Mobile Applications. (Guizhou University, University of Florida)
- Potluck: Cross-application Approximate Deduplication for Computation-Intensive Mobile Applications. (Yale)

## 2019
- FA3C: FPGA-Accelerated Deep Reinforcement Learning. (Hongik University, SNU)
- PUMA: A Programmable Ultra-efficient Memristor-based Accelerator for Machine Learning Inference. (Purdue, UIUC, HP)
- FPSA: A Full System Stack Solution for Reconfigurable ReRAM-based NN Accelerator Architecture. (THU, UCSB)
- Bit-Tactical: A Software/Hardware Approach to Exploiting Value and Bit Sparsity in Neural Networks. (Toronto, NVIDIA)
- TANGRAM: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators. (Stanford)
- Packing Sparse Convolutional Neural Networks for Efficient Systolic Array Implementations: Column Combining Under Joint Optimization. (Harvard)
- Split-CNN: Splitting Window-based Operations in Convolutional Neural Networks for Memory System Optimization. (IBM, Kyungpook National University)
- HOP: Heterogeneity-Aware Decentralized Training. (USC, THU)
- Astra: Exploiting Predictability to Optimize Deep Learning. (Microsoft)
- ADMM-NN: An Algorithm-Hardware Co-Design Framework of DNNs Using Alternating Direction Methods of Multipliers. (Northeastern, Syracuse, SUNY, Buffalo, USC)
- DeepSigns: An End-to-End Watermarking Framework for Protecting the Ownership of Deep Neural Networks. (UCSD)

## 2020
- Shredder: Learning Noise Distributions to Protect Inference Privacy.	(UCSD)
- DNNGuard: An Elastic Heterogeneous DNN Accelerator Architecture against Adversarial Attacks.	(CAS, USC)
- Interstellar: Using Halide’s Scheduling Language to Analyze DNN Accelerators.	(Stanford, THU)
- DeepSniffer: A DNN Model Extraction Framework Based on Learning Architectural Hints.	(UCSB)
- Prague: High-Performance Heterogeneity-Aware Asynchronous Decentralized Training.	(USC)
- PatDNN: Achieving Real-Time DNN Execution on Mobile Devices with Pattern-based Weight Pruning.	(College of William and Mary, Northeastern , USC)
- Capuchin: Tensor-based GPU Memory Management for Deep Learning.	(HUST, MSRA, USC)
- NeuMMU: Architectural Support for Efficient Address Translations in Neural Processing Units.	(KAIST)
- FlexTensor: An Automatic Schedule Exploration and Optimization Framework for Tensor Computation on Heterogeneous System.	(PKU)

# MICRO
## 2014
- DaDianNao: A Machine-Learning Supercomputer. (CAS, Inria, Inner Mongolia University)

## 2016
- From High-Level Deep Neural Models to FPGAs. (Georgia Institute of Technology, Intel)
- vDNN: Virtualized Deep Neural Networks for Scalable, Memory-Efficient Neural Network Design. (NVIDIA)
- Stripes: Bit-Serial Deep Neural Network Computing. (University of Toronto, University of British Columbia)
- Cambricon-X: An Accelerator for Sparse Neural Networks. (Chinese Academy of Sciences)
- NEUTRAMS: Neural Network Transformation and Co-design under Neuromorphic Hardware Constraints. (Tsinghua University, UCSB)
- Fused-Layer CNN Accelerators. (Stony Brook University)
- Bridging the I/O Performance Gap for Big Data Workloads: A New NVDIMM-based Approach. (The Hong Kong Polytechnic University, NSF/University of Florida)
- A Patch Memory System For Image Processing and Computer Vision. (NVIDIA)
- An Ultra Low-Power Hardware Accelerator for Automatic Speech Recognition. (Universitat Politecnica de Catalunya)
- Perceptron Learning for Reuse Prediction. (TAMU, Intel Labs)
- A Cloud-Scale Acceleration Architecture. (Microsoft Research)
- Reducing Data Movement Energy via Online Data Clustering and Encoding. (University of Rochester)
- The Microarchitecture of a Real-time Robot Motion Planning Accelerator. (Duke University)
- Chameleon: Versatile and Practical Near-DRAM Acceleration Architecture for Large Memory Systems. (UIUC, Seoul National University)

## 2017
- Bit-Pragmatic Deep Neural Network Computing. (NVIDIA, University of Toronto)
- CirCNN: Accelerating and Compressing Deep Neural Networks Using Block-Circulant Weight Matrices. (Syracuse University, City University of New York, USC, California State University, Northeastern University)
- DRISA: A DRAM-based Reconfigurable In-Situ Accelerator. (UCSB, Samsung)
- Scale-Out Acceleration for Machine Learning. (Georgia Tech, UCSD)
  - Propose CoSMIC, a full computing stack constituting language, compiler, system software, template architecture, and circuit generators, that enable programmable acceleration of learning at scale.
- DeftNN: Addressing Bottlenecks for DNN Execution on GPUs via Synapse Vector Elimination and Near-compute Data Fission. (Univ. of Michigan, Univ. of Nevada)
- Data Movement Aware Computation Partitioning. (PSU, TOBB University of Economics and Technology)
  - *Partition computation on a manycore system for near data processing.*

## 2018
- Addressing Irregularity in Sparse Neural Networks: A Cooperative Software/Hardware Approach. (USTC, CAS)
- Diffy: a Deja vu-Free Differential Deep Neural Network Accelerator. (University of Toronto)
- Beyond the Memory Wall: A Case for Memory-centric HPC System for Deep Learning. (KAIST)
- Towards Memory Friendly Long-Short Term Memory Networks (LSTMs) on Mobile GPUs. (University of Houston, Capital Normal University)
- A Network-Centric Hardware/Algorithm Co-Design to Accelerate Distributed Training of Deep Neural Networks. (UIUC, THU, SJTU, Intel, UCSD)
- PermDNN: Efficient Compressed Deep Neural Network Architecture with Permuted Diagonal Matrices. (City University of New York, University of Minnesota, USC)
- GeneSys: Enabling Continuous Learning through Neural Network Evolution in Hardware. (Georgia Tech)
- Processing-in-Memory for Energy-efficient Neural Network Training: A Heterogeneous Approach. (UCM, UCSD, UCSC)
- LerGAN: A Zero-free, Low Data Movement and PIM-based GAN Architecture. (THU, University of Florida)
- Multi-dimensional Parallel Training of Winograd Layer through Distributed Near-Data Processing. (KAIST)
- SCOPE: A Stochastic Computing Engine for DRAM-based In-situ Accelerator. (UCSB, Samsung)
- Morph: Flexible Acceleration for 3D CNN-based Video Understanding. (UIUC)
- Inter-thread Communication in Multithreaded, Reconfigurable Coarse-grain Arrays. (Technion)
- An Architectural Framework for Accelerating Dynamic Parallel Algorithms on Reconfigurable Hardware. (Cornell)

## 2019
- Wire-Aware Architecture and Dataflow for CNN Accelerators. (Utah)
- ShapeShifter: Enabling Fine-Grain Data Width Adaptation in Deep Learning. (Toronto)
- Simba: Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture. (NVIDIA)
- ZCOMP: Reducing DNN Cross-Layer Memory Footprint Using Vector Extensions. (Google, Intel)
- Boosting the Performance of CNN Accelerators with Dynamic Fine-Grained Channel Gating. (Cornell)
- SparTen: A Sparse Tensor Accelerator for Convolutional Neural Networks. (Purdue)
- EDEN: Enabling Approximate DRAM for DNN Inference using Error-Resilient Neural Networks. (ETHZ, CMU)
- eCNN: a Block-Based and Highly-Parallel CNN Accelerator for Edge Inference. (NTHU)
- TensorDIMM: A Practical Near-Memory Processing Architecture for Embeddings and Tensor Operations in Deep Learning. (KAIST)
- Understanding Reuse, Performance, and Hardware Cost of DNN Dataflows: A Data-Centric Approach. (Georgia Tech, NVIDIA)
- MaxNVM: Maximizing DNN Storage Density and Inference Efficiency with Sparse Encoding and Error Mitigation. (Harvard, Facebook)
- Neuron-Level Fuzzy Memoization in RNNs. (UPC)
- Manna: An Accelerator for Memory-Augmented Neural Networks. (Purdue, Intel)
- eAP: A Scalable and Efficient In-Memory Accelerator for Automata Processing. (Virginia)
- ComputeDRAM: In-Memory Compute Using Off-the-Shelf DRAMs. (Princeton)
- ExTensor: An Accelerator for Sparse Tensor Algebra. (UIUC, NVIDIA)
- Efficient SpMV Operation for Large and Highly Sparse Matrices Using Scalable Multi-Way Merge Parallelization. (CMU)
- Sparse Tensor Core: Algorithm and Hardware Co-Design for Vector-wise Sparse Neural Networks on Modern GPUs. (UCSB, Alibaba)
- DynaSprint: Microarchitectural Sprints with Dynamic Utility and Thermal Management. (Waterloo, ARM, Duke)
- MEDAL: Scalable DIMM based Near Data Processing Accelerator for DNA Seeding Algorithm. (UCSB, ICT)
- Tigris: Architecture and Algorithms for 3D Perception in Point Clouds. (Rochester)
- ASV: Accelerated Stereo Vision System. (Rochester)
- Alleviating Irregularity in Graph Analytics Acceleration: a Hardware/Software Co-Design Approach. (UCSB, ICT)

# HPCA
## 2016
- A Performance Analysis Framework for Optimizing OpenCL Applications on FPGAs. (Nanyang Technological University, HKUST, Cornell University)
- TABLA: A Unified Template-based Architecture for Accelerating Statistical Machine Learning. (Georgia Institute of Technology)
- Memristive Boltzmann Machine: A Hardware Accelerator for Combinatorial Optimization and Deep Learning. (University of Rochester)

## 2017
- FlexFlow: A Flexible Dataflow Accelerator Architecture for Convolutional Neural Networks. (Chinese Academy of Sciences)
- PipeLayer: A Pipelined ReRAM-Based Accelerator for Deep Learning. (University of Pittsburgh, University of Southern California)
- Towards Pervasive and User Satisfactory CNN across GPU Microarchitectures. (University of Florida)
- Supporting Address Translation for Accelerator-Centric Architectures. (UCLA)

## 2018
- Making Memristive Neural Network Accelerators Reliable. (University of Rochester)
- Towards Efficient Microarchitectural Design for Accelerating Unsupervised GAN-based Deep Learning. (University of Florida)
- Compressing DMA Engine: Leveraging Activation Sparsity for Training Deep Neural Networks. (POSTECH, NVIDIA, UT-Austin)
- In-situ AI: Towards Autonomous and Incremental Deep Learning for IoT Systems. (University of Florida, Chongqing University, Capital Normal University)
- RC-NVM: Enabling Symmetric Row and Column Memory Accesses for In-Memory Databases. (PKU, NUDT, Duke, UCLA, PSU)
- GraphR: Accelerating Graph Processing Using ReRAM. (Duke, USC, Binghamton University SUNY)
- GraphP: Reducing Communication of PIM-based Graph Processing with Efficient Data Partition. (THU, USC, Stanford)
- PM3: Power Modeling and Power Management for Processing-in-Memory. (PKU)

## 2019
- HyPar: Towards Hybrid Parallelism for Deep Learning Accelerator Array. (Duke, USC)
- E-RNN: Design Optimization for Efficient Recurrent Neural Networks in FPGAs. (Syracuse University, Northeastern University, Florida International University, USC, University at Buffalo)
- Bit Prudent In-Cache Acceleration of Deep Convolutional Neural Networks. (Michigan, Intel)
- Shortcut Mining: Exploiting Cross-layer Shortcut Reuse in DCNN Accelerators. (OSU)
- NAND-Net: Minimizing Computational Complexity of In-Memory Processing for Binary Neural Networks. (KAIST)
- Kelp: QoS for Accelerators in Machine Learning Platforms. (Microsoft, Google, UT Austin)
- Machine Learning at Facebook: Understanding Inference at the Edge. (Facebook)
- The Accelerator Wall: Limits of Chip Specialization. (Princeton)

## 2020
- Deep Learning Acceleration with Neuron-to-Memory Transformation.	(UCSD)
- HyGCN: A GCN Accelerator with Hybrid Architecture.	(ICT, UCSB)
- SIGMA: A Sparse and Irregular GEMM Accelerator with Flexible Interconnects for DNN Training.	(Georgia Tech)
- PREMA: A Predictive Multi-task Scheduling Algorithm For Preemptible NPUs.	(KAIST)
- ALRESCHA: A Lightweight Reconfigurable Sparse-Computation Accelerator.	(Georgia Tech)
- SpArch: Efficient Architecture for Sparse Matrix Multiplication.	(MIT, NVIDIA)
- A3: Accelerating Attention Mechanisms in Neural Networks with Approximation.	(SNU)
- AccPar: Tensor Partitioning for Heterogeneous Deep Learning Accelerator Arrays.	(Duke, USC)
- PIXEL: Photonic Neural Network Accelerator.	(Ohio, George Washington)
- The Architectural Implications of Facebook’s DNN-based Personalized Recommendation.	(Facebook)
- Enabling Highly Efficient Capsule Networks Processing Through A PIM-Based Architecture Design.	(Houston)
- Missing the Forest for the Trees: End-to-End AI Application Performance in Edge Data.	(UT Austin, Intel)
- Communication Lower Bound in Convolution Accelerators.	(ICT, THU)
- Fulcrum: a Simplified Control and Access Mechanism toward Flexible and Practical in-situ Accelerators.	(Virginia, UCSB, Micron)
- EFLOPS: Algorithm and System Co-design for a High Performance Distributed Training Platform.	(Alibaba)
- Experiences with ML-Driven Design: A NoC Case Study.	(AMD)
- Tensaurus: A Versatile Accelerator for Mixed Sparse-Dense Tensor Computations.	(Cornell, Intel)
- A Hybrid Systolic-Dataflow Architecture for Inductive Matrix Algorithms.	(UCLA)
- A Deep Reinforcement Learning Framework for Architectural Exploration: A Routerless NoC Case Study.	(USC, OSU)
- QuickNN: Memory and Performance Optimization of k-d Tree Based Nearest Neighbor Search for 3D Point Clouds.	(Umich, General Motors)
- Orbital Edge Computing: Machine Inference in Space.	(CMU)
- A Scalable and Efficient in-Memory Interconnect Architecture for Automata Processing.	(Virginia)
- Techniques for Reducing the Connected-Standby Energy Consumption of Mobile Devices.	(ETHZ, Cyprus, CMU)
